{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "157622cd",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [8]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd526c00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T10:04:25.769610Z",
     "iopub.status.busy": "2024-10-03T10:04:25.769122Z",
     "iopub.status.idle": "2024-10-03T10:04:25.794747Z",
     "shell.execute_reply": "2024-10-03T10:04:25.793509Z"
    },
    "papermill": {
     "duration": 0.038911,
     "end_time": "2024-10-03T10:04:25.797519",
     "exception": false,
     "start_time": "2024-10-03T10:04:25.758608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INTERACTIVE_MODE = True  # Set to True to run the notebook interactively\n",
    "\n",
    "import sys\n",
    "\n",
    "if INTERACTIVE_MODE:\n",
    "    sys.path.append(\"./src\")\n",
    "    %load_ext autoreload\n",
    "    %autoreload 3\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    sys.path.append(\"./src\")\n",
    "    from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a00b7016",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T10:04:25.818565Z",
     "iopub.status.busy": "2024-10-03T10:04:25.818182Z",
     "iopub.status.idle": "2024-10-03T10:04:29.707409Z",
     "shell.execute_reply": "2024-10-03T10:04:29.706516Z"
    },
    "papermill": {
     "duration": 3.899548,
     "end_time": "2024-10-03T10:04:29.710492",
     "exception": false,
     "start_time": "2024-10-03T10:04:25.810944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import itertools\n",
    "from random import shuffle\n",
    "\n",
    "_ = th.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1eefd9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T10:04:29.726317Z",
     "iopub.status.busy": "2024-10-03T10:04:29.725872Z",
     "iopub.status.idle": "2024-10-03T10:04:29.730950Z",
     "shell.execute_reply": "2024-10-03T10:04:29.729901Z"
    },
    "papermill": {
     "duration": 0.014867,
     "end_time": "2024-10-03T10:04:29.732772",
     "exception": false,
     "start_time": "2024-10-03T10:04:29.717905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_name = \"obj_patch_translation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69b455c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T10:04:29.744787Z",
     "iopub.status.busy": "2024-10-03T10:04:29.744386Z",
     "iopub.status.idle": "2024-10-03T10:04:29.750677Z",
     "shell.execute_reply": "2024-10-03T10:04:29.749570Z"
    },
    "papermill": {
     "duration": 0.015601,
     "end_time": "2024-10-03T10:04:29.753224",
     "exception": false,
     "start_time": "2024-10-03T10:04:29.737623",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# papermill parameters\n",
    "batch_size = 8\n",
    "model = \"meta-llama/Llama-2-7b-hf\"\n",
    "model_path = None\n",
    "trust_remote_code = False\n",
    "device = \"auto\"\n",
    "remote = False\n",
    "num_few_shot = 5\n",
    "exp_id = None\n",
    "extra_args = []\n",
    "use_tl = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95a1b390",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T10:04:29.766327Z",
     "iopub.status.busy": "2024-10-03T10:04:29.765896Z",
     "iopub.status.idle": "2024-10-03T10:04:29.772107Z",
     "shell.execute_reply": "2024-10-03T10:04:29.770926Z"
    },
    "papermill": {
     "duration": 0.014835,
     "end_time": "2024-10-03T10:04:29.774331",
     "exception": false,
     "start_time": "2024-10-03T10:04:29.759496",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "model = \"bigscience/bloom-7b1\"\n",
    "model_path = None\n",
    "remote = False\n",
    "trust_remote_code = False\n",
    "device = \"auto\"\n",
    "batch_size = 8\n",
    "num_few_shot = 5\n",
    "use_tl = False\n",
    "exp_id = \"test\"\n",
    "extra_args = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0560b1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T10:04:29.787283Z",
     "iopub.status.busy": "2024-10-03T10:04:29.786687Z",
     "iopub.status.idle": "2024-10-03T10:04:37.320087Z",
     "shell.execute_reply": "2024-10-03T10:04:37.319008Z"
    },
    "papermill": {
     "duration": 7.543287,
     "end_time": "2024-10-03T10:04:37.323097",
     "exception": false,
     "start_time": "2024-10-03T10:04:29.779810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nnterp import load_model\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--num-patches\", type=int, default=-1)\n",
    "pargs = parser.parse_args(extra_args)\n",
    "num_patches = pargs.num_patches\n",
    "\n",
    "if model_path is None:\n",
    "    model_path = model\n",
    "nn_model = load_model(\n",
    "    model_path,\n",
    "    trust_remote_code=trust_remote_code,\n",
    "    no_space_on_bos=True,\n",
    "    device_map=device,\n",
    "    use_tl=use_tl,\n",
    ")\n",
    "tokenizer = nn_model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce0c36c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomForCausalLM(\n",
       "  (model): BloomModel(\n",
       "    (word_embeddings): Embedding(250880, 4096)\n",
       "    (word_embeddings_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x BloomBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_final): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=250880, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3888d306",
   "metadata": {
    "papermill": {
     "duration": 0.006849,
     "end_time": "2024-10-03T10:04:37.337521",
     "exception": false,
     "start_time": "2024-10-03T10:04:37.330672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f22f4d18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T10:04:37.349625Z",
     "iopub.status.busy": "2024-10-03T10:04:37.349336Z",
     "iopub.status.idle": "2024-10-03T10:04:37.938103Z",
     "shell.execute_reply": "2024-10-03T10:04:37.937118Z"
    },
    "papermill": {
     "duration": 0.59858,
     "end_time": "2024-10-03T10:04:37.941330",
     "exception": false,
     "start_time": "2024-10-03T10:04:37.342750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from exp_tools import (\n",
    "    run_prompts,\n",
    ")\n",
    "from interventions import (\n",
    "    object_lens,\n",
    "    collect_activations,\n",
    "    collect_activations_batched,\n",
    "    get_num_layers,\n",
    ")\n",
    "from prompt_tools import translation_prompts, get_obj_id\n",
    "from load_dataset import get_word_translation_dataset as get_translations\n",
    "\n",
    "from utils import ulist\n",
    "from display_utils import plot_topk_tokens, plot_results, plot_k_results, k_subplots\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def object_patching_plot(\n",
    "    source_lang_pairs,\n",
    "    input_lang,\n",
    "    target_lang,\n",
    "    extra_langs=None,\n",
    "    batch_size=batch_size,\n",
    "    num_words=None,\n",
    "    num_pairs=200,\n",
    "    exp_id=exp_id,\n",
    "    k=4,\n",
    "    remote=remote,\n",
    "):\n",
    "    \"\"\"\n",
    "    Experiment 2 of the paper:\n",
    "    - For each source_lang_pairs, construct a prompt translating the same concept (e.g. DOG):\n",
    "    L1: \"CAT^L1\" - L2: \"CAT^L2\"\n",
    "    ...\n",
    "    L1: \"DOG^L1\n",
    "\n",
    "    - Collect activation at the last token of the prompt and generate a mean latent representation for each layer\n",
    "\n",
    "    - For each layer `j`: Run the target prompts which are translations from the input_lang to the target_lang. During the forward pass, patch at the last token of the concept to be translated with the mean latent representation of the source prompts from `j` to the last layer.\n",
    "\n",
    "    We plot both the probabilities you get from the mean latent and the probabilities you get from the first source_lang_pairs latent.\n",
    "    \"\"\"\n",
    "    source_lang_pairs = np.array(source_lang_pairs)\n",
    "    if extra_langs is None:\n",
    "        extra_langs = []\n",
    "    if isinstance(extra_langs, str):\n",
    "        extra_langs = [extra_langs]\n",
    "    model_name = model.split(\"/\")[-1]\n",
    "    global source_df, target_df, target_prompts, target_probs, latent_probs, source_prompts, _source_prompts, _target_prompts\n",
    "    if exp_id is None:\n",
    "        exp_id = str(int(time()))\n",
    "    else:\n",
    "        exp_id = str(exp_id)\n",
    "    source_df = get_translations(\n",
    "        \"en\",\n",
    "        ulist([*source_lang_pairs.flatten(), input_lang, target_lang, *extra_langs]),\n",
    "        num_words,\n",
    "    )\n",
    "    target_df = get_translations(\n",
    "        input_lang,\n",
    "        ulist([*source_lang_pairs.flatten(), target_lang, *extra_langs]),\n",
    "        num_words,\n",
    "    )\n",
    "\n",
    "    _source_prompts = list(\n",
    "        zip(\n",
    "            *[\n",
    "                translation_prompts(\n",
    "                    source_df,\n",
    "                    nn_model.tokenizer,\n",
    "                    inp_lang,\n",
    "                    targ_lang,\n",
    "                    [target_lang, *extra_langs],\n",
    "                    augment_tokens=False,\n",
    "                    n=num_few_shot,\n",
    "                )\n",
    "                for inp_lang, targ_lang in source_lang_pairs\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    _target_prompts = translation_prompts(\n",
    "        target_df,\n",
    "        nn_model.tokenizer,\n",
    "        input_lang,\n",
    "        target_lang,\n",
    "        [*extra_langs],  # [*list(zip(*source_lang_pairs))[1], *extra_langs],\n",
    "        augment_tokens=False,\n",
    "        n=num_few_shot,\n",
    "    )\n",
    "\n",
    "    collected_pairs = 0\n",
    "    source_prompts = []\n",
    "    target_prompts = []\n",
    "    source_target = list(itertools.product(source_df.iterrows(), target_df.iterrows()))\n",
    "    shuffle(source_target)\n",
    "\n",
    "    for (i, source_row), (j, target_row) in source_target:\n",
    "        if source_row[\"word_original\"] == target_row[\"word_original\"]:\n",
    "            continue\n",
    "        src_p = _source_prompts[i]\n",
    "        targ_p = deepcopy(_target_prompts[j])\n",
    "        latent_tokens = {f\"source_{target_lang}\": src_p[0].latent_tokens[target_lang]}\n",
    "        latent_tokens.update(**targ_p.latent_tokens)\n",
    "        targ_p.latent_tokens = latent_tokens\n",
    "        targ_p.latent_strings[f\"sources\"] = ulist(\n",
    "            sum([p.target_strings for p in src_p], [])\n",
    "        )\n",
    "        targ_p.latent_strings[f\"source_{target_lang}\"] = src_p[0].latent_strings[\n",
    "            target_lang\n",
    "        ]\n",
    "        for lang in extra_langs:\n",
    "            targ_p.latent_tokens[f\"src + tgt {lang}\"] = ulist(\n",
    "                targ_p.latent_tokens[lang] + src_p[0].latent_tokens[lang]\n",
    "            )\n",
    "            targ_p.latent_strings[f\"src + tgt {lang}\"] = ulist(\n",
    "                targ_p.latent_strings[lang] + src_p[0].latent_strings[lang]\n",
    "            )\n",
    "            del targ_p.latent_tokens[lang]\n",
    "        if targ_p.has_no_collisions():\n",
    "            source_prompts.append(src_p)\n",
    "            target_prompts.append(targ_p)\n",
    "            collected_pairs += 1\n",
    "        if collected_pairs >= num_pairs:\n",
    "            break\n",
    "    if collected_pairs < num_pairs:\n",
    "        print(\n",
    "            f\"Could only collect {collected_pairs} pairs for {source_lang_pairs.tolist()} - {input_lang} -> {target_lang}, skipping...\"\n",
    "        )\n",
    "        return\n",
    "    source_prompts = np.array(source_prompts)\n",
    "    source_prompts_str = np.array(\n",
    "        [['\"'.join(p.prompt.split('\"')[:-2]) for p in ps] for ps in source_prompts]\n",
    "    )\n",
    "    idx = get_obj_id(target_prompts[0].prompt, nn_model.tokenizer)\n",
    "\n",
    "    def object_patching(\n",
    "        nn_model, prompt_batch, scan, source_prompt_batch=None, only_first=False\n",
    "    ):\n",
    "        offset = object_patching.offset\n",
    "        batch_size = len(prompt_batch)\n",
    "        if source_prompt_batch is None:\n",
    "            source_prompt_batch = source_prompts_str[offset : offset + batch_size]\n",
    "        if only_first:\n",
    "            source_prompt_batch = source_prompt_batch[:, :1]\n",
    "        hiddens = collect_activations_batched(\n",
    "            nn_model,\n",
    "            source_prompt_batch.flatten(),\n",
    "            batch_size=batch_size,\n",
    "            remote=remote,\n",
    "        )\n",
    "        hiddens = hiddens.transpose(0, 1)  # (all_prompts, layer, hidden_size)\n",
    "        hiddens = hiddens.reshape(\n",
    "            batch_size, source_prompt_batch.shape[1], get_num_layers(nn_model), -1\n",
    "        ).mean(\n",
    "            dim=1\n",
    "        )  # (batch_size, num_layers, hidden_size)\n",
    "        hiddens = hiddens.transpose(0, 1)  # (num_layers, batch_size, hidden_size)\n",
    "        object_patching.offset += batch_size\n",
    "        return object_lens(\n",
    "            nn_model,\n",
    "            prompt_batch,\n",
    "            idx,\n",
    "            hiddens=hiddens,\n",
    "            scan=scan,\n",
    "            num_patches=num_patches,\n",
    "            remote=remote,\n",
    "        )\n",
    "\n",
    "    object_patching.offset = 0\n",
    "    target_probs, latent_probs = run_prompts(\n",
    "        nn_model,\n",
    "        target_prompts,\n",
    "        batch_size=batch_size,\n",
    "        get_probs=object_patching,\n",
    "        tqdm=tqdm,\n",
    "    )\n",
    "\n",
    "    object_patching.offset = 0\n",
    "    of_target_probs, of_latent_probs = run_prompts(\n",
    "        nn_model,\n",
    "        target_prompts,\n",
    "        batch_size=batch_size,\n",
    "        get_probs=object_patching,\n",
    "        get_probs_kwargs=dict(only_first=True),\n",
    "        tqdm=tqdm,\n",
    "    )\n",
    "\n",
    "    # Get the baseline to normalize the plots\n",
    "    all_source_prompts = source_prompts.flatten()\n",
    "    source_prompts_probs = (\n",
    "        run_prompts(\n",
    "            nn_model,\n",
    "            all_source_prompts,\n",
    "            batch_size=batch_size,\n",
    "            get_probs_kwargs=dict(remote=remote),\n",
    "            tqdm=tqdm,\n",
    "        )[0]\n",
    "        .squeeze()\n",
    "        .reshape(len(source_prompts), -1)\n",
    "    )\n",
    "\n",
    "    target_prompts_probs, _ = run_prompts(\n",
    "        nn_model,\n",
    "        target_prompts,\n",
    "        batch_size=batch_size,\n",
    "        get_probs_kwargs=dict(remote=remote),\n",
    "        tqdm=tqdm,\n",
    "    )\n",
    "\n",
    "    json_dic = {\n",
    "        target_lang: target_probs.tolist(),\n",
    "        \"source prompt probs\": source_prompts_probs.squeeze().tolist(),\n",
    "        \"target prompt probs\": target_prompts_probs.squeeze().tolist(),\n",
    "    }\n",
    "    for label, probs in latent_probs.items():\n",
    "        json_dic[label] = probs.tolist()\n",
    "    json_dic[\"only first\"] = {target_lang: of_target_probs.tolist()}\n",
    "    for label, probs in of_latent_probs.items():\n",
    "        json_dic[\"only first\"][label] = probs.tolist()\n",
    "    pref = \"_\".join(\"-\".join(ls) for ls in source_lang_pairs)\n",
    "    path = (\n",
    "        Path(\"results\")\n",
    "        / model_name\n",
    "        / exp_name\n",
    "        / (f\"{pref}-{input_lang}_{target_lang}-\")\n",
    "    )\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    json_file = path / (exp_id + \".json\")\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(json_dic, f, indent=4)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    pref = pref.replace(\"_\", \" \")\n",
    "    title = f\"{model_name}: ObjPatch from ({pref}) into ({input_lang} -> {target_lang})\"\n",
    "    plot_results(\n",
    "        ax,\n",
    "        target_probs,\n",
    "        latent_probs,\n",
    "        target_lang,\n",
    "        source_baseline=source_prompts_probs.mean(),\n",
    "        target_baseline=target_prompts_probs.mean(),\n",
    "    )\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    plot_file = path / (exp_id + \".png\")\n",
    "    plt.savefig(plot_file, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    plot_results(ax, of_target_probs, of_latent_probs, target_lang)\n",
    "    ax.legend()\n",
    "    pref2 = pref.split(\" \")[0]\n",
    "    ax.set_title(\n",
    "        f\"{model_name}: ObjPatch from ({pref2}) into ({input_lang} -> {target_lang}\"\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plot_file = path / (exp_id + \"_only_first.png\")\n",
    "    plt.savefig(plot_file, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot k examples\n",
    "    fig, axes = k_subplots(k)\n",
    "    plot_k_results(axes, target_probs, latent_probs, target_lang, k)\n",
    "    axes[k - 1].legend()\n",
    "    fig.suptitle(title)\n",
    "    plt_file = path / (exp_id + \"_k.png\")\n",
    "    fig.savefig(plt_file, dpi=300, bbox_inches=\"tight\")\n",
    "    fig.show()\n",
    "    # Compute a single example\n",
    "    json_meta = {}\n",
    "    for i in range(k):\n",
    "        json_meta[i] = {\n",
    "            \"source pairs\": source_lang_pairs.tolist(),\n",
    "            \"input lang\": input_lang,\n",
    "            \"target lang\": target_lang,\n",
    "            \"source prompt\": {\n",
    "                \"-\".join(l) + \" \" + str(j): source_prompts_str[i][j]\n",
    "                for j, l in enumerate(source_lang_pairs)\n",
    "            },\n",
    "            \"source prompt target\": {\n",
    "                \"-\".join(l) + \" \" + str(j): source_prompts[i][j].target_strings\n",
    "                for j, l in enumerate(source_lang_pairs)\n",
    "            },\n",
    "            \"source prompt latent\": {\n",
    "                \"-\".join(l) + \" \" + str(j): source_prompts[i][j].latent_strings\n",
    "                for j, l in enumerate(source_lang_pairs)\n",
    "            },\n",
    "            \"target prompt\": target_prompts[i].prompt,\n",
    "            \"target prompt target\": target_prompts[i].target_strings,\n",
    "            \"target prompt latent\": target_prompts[i].latent_strings,\n",
    "        }\n",
    "    json_df = pd.DataFrame(json_meta)\n",
    "    with pd.option_context(\n",
    "        \"display.max_colwidth\",\n",
    "        None,\n",
    "        \"display.max_columns\",\n",
    "        None,\n",
    "        \"display.max_rows\",\n",
    "        None,\n",
    "    ):\n",
    "        display(json_df)\n",
    "    target_prompt_batch = [p.prompt for p in target_prompts[:k]]\n",
    "    probs = object_patching(\n",
    "        nn_model,\n",
    "        target_prompt_batch,\n",
    "        scan=True,\n",
    "        source_prompt_batch=source_prompts_str[:k],\n",
    "    )\n",
    "    file = path / (exp_id + \"_heatmap.png\")\n",
    "    plot_topk_tokens(probs, nn_model, title=title, file=file)\n",
    "\n",
    "    meta_file = path / (exp_id + \"_heatmap.meta.json\")\n",
    "    with open(meta_file, \"w\") as f:\n",
    "        json.dump(json_meta, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb340201",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5be182b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T10:04:37.957315Z",
     "iopub.status.busy": "2024-10-03T10:04:37.956920Z",
     "iopub.status.idle": "2024-10-03T10:04:41.827000Z",
     "shell.execute_reply": "2024-10-03T10:04:41.825541Z"
    },
    "papermill": {
     "duration": 3.879834,
     "end_time": "2024-10-03T10:04:41.829234",
     "exception": true,
     "start_time": "2024-10-03T10:04:37.949400",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Running prompts:   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Running prompts:   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BloomForCausalLM' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m paper_args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     [[(l, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m paper_ins], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124met\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m     (paper_pairs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzh\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pargs \u001b[38;5;129;01min\u001b[39;00m paper_args:\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mobject_patching_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_langs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 170\u001b[0m, in \u001b[0;36mobject_patching_plot\u001b[0;34m(source_lang_pairs, input_lang, target_lang, extra_langs, batch_size, num_words, num_pairs, exp_id, k, remote)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m object_lens(\n\u001b[1;32m    160\u001b[0m         nn_model,\n\u001b[1;32m    161\u001b[0m         prompt_batch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m         remote\u001b[38;5;241m=\u001b[39mremote,\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    169\u001b[0m object_patching\u001b[38;5;241m.\u001b[39moffset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 170\u001b[0m target_probs, latent_probs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_prompts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnn_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_patching\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m object_patching\u001b[38;5;241m.\u001b[39moffset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    179\u001b[0m of_target_probs, of_latent_probs \u001b[38;5;241m=\u001b[39m run_prompts(\n\u001b[1;32m    180\u001b[0m     nn_model,\n\u001b[1;32m    181\u001b[0m     target_prompts,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m     tqdm\u001b[38;5;241m=\u001b[39mtqdm,\n\u001b[1;32m    186\u001b[0m )\n",
      "File \u001b[0;32m/dlabscratch1/cdumas/thinking-lang/.conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dlabscratch1/cdumas/thinking-lang/./src/exp_tools.py:125\u001b[0m, in \u001b[0;36mrun_prompts\u001b[0;34m(nn_model, prompts, batch_size, get_probs, get_probs_kwargs, scan, tqdm)\u001b[0m\n\u001b[1;32m    123\u001b[0m     tqdm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: x\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt_batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 125\u001b[0m     probs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mget_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mget_probs_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m     scan \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Not sure if this is a good idea\u001b[39;00m\n\u001b[1;32m    127\u001b[0m probs \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mcat(probs)\n",
      "Cell \u001b[0;32mIn[7], line 145\u001b[0m, in \u001b[0;36mobject_patching_plot.<locals>.object_patching\u001b[0;34m(nn_model, prompt_batch, scan, source_prompt_batch, only_first)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m only_first:\n\u001b[1;32m    144\u001b[0m     source_prompt_batch \u001b[38;5;241m=\u001b[39m source_prompt_batch[:, :\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 145\u001b[0m hiddens \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_activations_batched\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnn_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_prompt_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m hiddens \u001b[38;5;241m=\u001b[39m hiddens\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (all_prompts, layer, hidden_size)\u001b[39;00m\n\u001b[1;32m    152\u001b[0m hiddens \u001b[38;5;241m=\u001b[39m hiddens\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    153\u001b[0m     batch_size, source_prompt_batch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], get_num_layers(nn_model), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    154\u001b[0m )\u001b[38;5;241m.\u001b[39mmean(\n\u001b[1;32m    155\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    156\u001b[0m )  \u001b[38;5;66;03m# (batch_size, num_layers, hidden_size)\u001b[39;00m\n",
      "File \u001b[0;32m/dlabscratch1/cdumas/thinking-lang/./src/nnsight_utils.py:238\u001b[0m, in \u001b[0;36mcollect_activations_batched\u001b[0;34m(nn_model, prompts, batch_size, layers, get_activations, remote, idx, tqdm)\u001b[0m\n\u001b[1;32m    236\u001b[0m acts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m--> 238\u001b[0m     acts_batch \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_activations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_activations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     acts\u001b[38;5;241m.\u001b[39mappend(acts_batch)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m th\u001b[38;5;241m.\u001b[39mcat(acts, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/dlabscratch1/cdumas/thinking-lang/.conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dlabscratch1/cdumas/thinking-lang/./src/nnsight_utils.py:186\u001b[0m, in \u001b[0;36mcollect_activations\u001b[0;34m(nn_model, prompts, layers, get_activations, remote, idx, open_context)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositive index is currently not supported due to left padding\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    184\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[43mget_num_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_model\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(h):\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_context:\n",
      "File \u001b[0;32m/dlabscratch1/cdumas/thinking-lang/./src/nnsight_utils.py:24\u001b[0m, in \u001b[0;36mget_num_layers\u001b[0;34m(nn_model)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(nn_model\u001b[38;5;241m.\u001b[39mblocks)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mlayers)\n",
      "File \u001b[0;32m/dlabscratch1/cdumas/thinking-lang/.conda/lib/python3.11/site-packages/nnsight/models/NNsightModel.py:541\u001b[0m, in \u001b[0;36mNNsight.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Envoy, InterventionProxy, Any]:\n\u001b[1;32m    536\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper of ._envoy's attributes to access module's inputs and outputs.\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m        Any: Attribute.\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_envoy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dlabscratch1/cdumas/thinking-lang/.conda/lib/python3.11/site-packages/nnsight/envoy.py:409\u001b[0m, in \u001b[0;36mEnvoy.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Envoy, Any]:\n\u001b[1;32m    400\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper method for underlying module's attributes.\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;124;03m        Any: Attribute.\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dlabscratch1/cdumas/thinking-lang/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BloomForCausalLM' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "paper_pairs = [(\"de\", \"it\"), (\"nl\", \"fi\"), (\"zh\", \"es\"), (\"es\", \"ru\"), (\"ru\", \"ko\")]\n",
    "paper_ins = [\"de\", \"nl\", \"zh\", \"es\", \"ru\"]\n",
    "paper_args = [\n",
    "    [[(l, \"hi\") for l in paper_ins], \"fr\", \"et\"],\n",
    "    (paper_pairs, \"fr\", \"zh\"),\n",
    "]\n",
    "for pargs in paper_args:\n",
    "    object_patching_plot(*pargs, extra_langs=[\"en\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20.592471,
   "end_time": "2024-10-03T10:04:44.185593",
   "environment_variables": {},
   "exception": true,
   "input_path": "/dlabscratch1/cdumas/thinking-lang/notebooks/obj_patch_translation.ipynb",
   "output_path": "/dlabscratch1/cdumas/thinking-lang/results/obj_patch_translation/bigscience_bloom-7b1_1727949863_almond-raven.ipynb",
   "parameters": {
    "batch_size": 8,
    "device": "auto",
    "exp_id": "1727949863_almond-raven",
    "extra_args": [],
    "model": "bigscience/bloom-7b1",
    "model_path": null,
    "num_few_shot": 5,
    "remote": false,
    "trust_remote_code": false,
    "use_tl": false
   },
   "start_time": "2024-10-03T10:04:23.593122",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
