{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import itertools\n",
    "import gc\n",
    "from random import shuffle\n",
    "\n",
    "_ = th.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"mean_repr_def\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# papermill parameters\n",
    "batch_size = 8\n",
    "model = \"google/gemma-2-2b\"\n",
    "model_path = None\n",
    "trust_remote_code = False\n",
    "device = \"auto\"\n",
    "remote = False\n",
    "num_few_shot = 5\n",
    "exp_id = \"test\"\n",
    "extra_args = []\n",
    "use_tl = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERACTIVE_MODE = exp_id == \"test\"  # Set to True to run the notebook interactively\n",
    "DEBUG = exp_id == \"test\" and True\n",
    "import sys\n",
    "if DEBUG:\n",
    "    print(\"Debugging...\")\n",
    "if INTERACTIVE_MODE:\n",
    "    print(\"Interactive mode!\")\n",
    "    exp_id = \"test\" + str(int(time()))\n",
    "    sys.path.append(\"../src\")\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from tqdm.notebook import tqdm, trange\n",
    "else:\n",
    "    sys.path.append(\"./src\")\n",
    "    from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnterp import load_model\n",
    " \n",
    "from argparse import ArgumentParser\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--gen-batch-size\", type=int, default=batch_size // 2)\n",
    "parser.add_argument(\n",
    "    \"--embeddings-model\",\n",
    "    type=str,\n",
    "    default=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    ")\n",
    "parser.add_argument(\"--emb-batch-size\", type=int, default=256)\n",
    "parser.add_argument(\"--debug\", action=\"store_true\")\n",
    "pargs = parser.parse_args(extra_args)\n",
    "DEBUG = pargs.debug\n",
    "if DEBUG:\n",
    "    print(\"/!\\\\ Debugging...\")\n",
    "    exp_name = \"debug-\" + exp_name\n",
    "gen_batch_size = pargs.gen_batch_size\n",
    "emb_batch_size = pargs.emb_batch_size\n",
    "embeddings_model = SentenceTransformer(\n",
    "    (pargs.embeddings_model), device=device if device != \"auto\" else None\n",
    ")\n",
    "\n",
    "if model_path is None:\n",
    "    model_path = (model)\n",
    "nn_model = load_model(\n",
    "    model_path,\n",
    "    trust_remote_code=trust_remote_code,\n",
    "    device_map=device,\n",
    "    use_tl=use_tl,\n",
    "    no_space_on_bos=True,\n",
    ")\n",
    "tokenizer = nn_model.tokenizer\n",
    "model_name = model.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnterp.nnsight_utils import (\n",
    "    collect_activations_batched,\n",
    "    get_num_layers,\n",
    "    get_layer_output,\n",
    ")\n",
    "from random import sample\n",
    "from load_dataset import get_word_translation_dataset, get_cloze_dataset, load_synset\n",
    "from utils import ulist\n",
    "from prompt_tools import translation_prompts, def_prompt, get_obj_id\n",
    "from torch.utils.data import DataLoader\n",
    "import ast\n",
    "import itertools\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from plot_utils import plot_defs_comparison, plot_compare_setup, plot_losses_comparison\n",
    "import torch.nn.functional as F\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def extract_def(generation):\n",
    "    match generation[-2], generation[-1]:\n",
    "        case '\"', \"\\n\":\n",
    "            pass\n",
    "        case _, \"\\n\":\n",
    "            end = generation.split(\"\\n\")[-2]\n",
    "            warnings.warn(f\"Last line does not end with a quote: {end}\")\n",
    "            generation = generation[:-1] + '\"\\n'\n",
    "        case _, '\"':\n",
    "            generation += \"\\n\"\n",
    "        case _:\n",
    "            end = generation.split(\"\\n\")[-1]\n",
    "            warnings.warn(f\"Last line does not end with a quote and newline: {end}\")\n",
    "            generation += '\"\\n'\n",
    "\n",
    "    last_line = generation.split(\"\\n\")[-2]\n",
    "    split = last_line.split('\"')\n",
    "    return '\"'.join(split[3:-1])\n",
    "\n",
    "\n",
    "def patched_generation(prompts, reprs, gen_batch_size):\n",
    "    pos = get_obj_id(prompts[0], tokenizer)\n",
    "    out = []\n",
    "    for i in trange(\n",
    "        0, len(prompts), gen_batch_size, desc=\"Generating patched generations\"\n",
    "    ):\n",
    "        end = min(i + gen_batch_size, len(prompts))\n",
    "        with nn_model.generate(\n",
    "            prompts[i:end],\n",
    "            max_new_tokens=50,\n",
    "            stop_strings=[\"\\n\"],\n",
    "            tokenizer=nn_model.tokenizer,\n",
    "        ):\n",
    "            if reprs is not None:\n",
    "                for layer in range(get_num_layers(nn_model)):\n",
    "                    get_layer_output(nn_model, layer)[:, pos] = reprs[i:end, layer]\n",
    "            out_model = nn_model.generator.output.tolist().save()\n",
    "        out.extend(out_model)\n",
    "    return out\n",
    "\n",
    "\n",
    "def loss_on_defs(prompts: list[str], defs: dict[str, list[str]], reprs=None):\n",
    "    \"\"\"\n",
    "    Compute the loss on the definitions\n",
    "    Args:\n",
    "        prompts: list of prompts for each concept\n",
    "        defs: dictionary of concept -> list of definitions\n",
    "        reprs: optional tensor of shape (num_prompts, num_layers, model_dim) containing the representations to patch\n",
    "    Returns:\n",
    "        dictionary of concept -> list of losses\n",
    "    \"\"\"\n",
    "    patch_pos = []\n",
    "    inputs = []\n",
    "    def_lengths = []\n",
    "    all_defs = list(itertools.chain.from_iterable(defs.values()))\n",
    "    all_prompts = list(\n",
    "        itertools.chain.from_iterable(\n",
    "            [p] * len(def_) for p, def_ in zip(prompts, defs.values())\n",
    "        )\n",
    "    )\n",
    "    repr_index = list(\n",
    "        itertools.chain.from_iterable(\n",
    "            [[i] * len(def_) for i, def_ in enumerate(defs.values())]\n",
    "        )\n",
    "    )\n",
    "    for prompt, def_ in zip(all_prompts, all_defs):\n",
    "        pos = get_obj_id(prompt, tokenizer)\n",
    "        def_ = def_ + '\"'\n",
    "        def_length = len(tokenizer.encode(def_, add_special_tokens=False))\n",
    "        def_lengths.append(def_length)\n",
    "        patch_pos.append(pos - def_length)\n",
    "        prompt_toks = tokenizer(prompt).input_ids\n",
    "        def_toks = tokenizer(def_, add_special_tokens=False).input_ids\n",
    "        input = {\n",
    "            \"input_ids\": prompt_toks + def_toks,\n",
    "            \"attention_mask\": [1] * (len(prompt_toks) + len(def_toks)),\n",
    "        }\n",
    "        inputs.append(input)\n",
    "    def_lengths = th.tensor(def_lengths)\n",
    "    def_losses = []\n",
    "    for i in trange(0, len(inputs), gen_batch_size, desc=\"Computing losses\"):\n",
    "        end = min(i + gen_batch_size, len(inputs))\n",
    "        batch_inputs = tokenizer.pad(inputs[i:end], return_tensors=\"pt\")\n",
    "        batch_patch_pos = th.tensor(patch_pos[i:end])\n",
    "        batch_repr_index = th.tensor(repr_index[i:end])\n",
    "        with nn_model.trace(batch_inputs):\n",
    "            if reprs is not None:\n",
    "                for layer in range(get_num_layers(nn_model)):\n",
    "                    get_layer_output(nn_model, layer)[\n",
    "                        th.arange(end - i), batch_patch_pos\n",
    "                    ] = reprs[batch_repr_index, layer]\n",
    "            logits = nn_model.output.logits.save()\n",
    "        # compute loss\n",
    "        mask = th.arange(logits.size(1)).unsqueeze(0) >= (\n",
    "            logits.size(1) - (def_lengths[i:end].unsqueeze(1))\n",
    "        )\n",
    "        assert mask.shape[0] == logits.shape[0] and mask.shape[1] == logits.shape[1]\n",
    "        logits = logits[:, :-1, :]\n",
    "        labels = batch_inputs.input_ids\n",
    "        labels = labels[:, 1:].to(logits.device)\n",
    "        labels[~mask[:, 1:]] = -100\n",
    "        loss = F.cross_entropy(logits.transpose(1, 2), labels, reduction=\"none\").sum(\n",
    "            dim=1\n",
    "        ) / mask[:, 1:].sum(\n",
    "            dim=1\n",
    "        ).to(logits.device)  # num_defs\n",
    "        assert loss.dim() == 1\n",
    "        def_losses.append(loss.cpu())\n",
    "    losses = th.cat(def_losses)\n",
    "    losses_dict = {}\n",
    "    prev_idx = 0\n",
    "    for concept in defs:\n",
    "        losses_dict[concept] = losses[prev_idx : prev_idx + len(defs[concept])].tolist()\n",
    "        prev_idx += len(defs[concept])\n",
    "    return losses_dict\n",
    "\n",
    "\n",
    "def patched_repr_defs(\n",
    "    lang_pairs, target_lang, path=None, num_other_for_loss=4, generate_generations=True\n",
    "):\n",
    "    lang_pairs = np.array(lang_pairs)\n",
    "    output_langs = ulist(lang_pairs[:, 1])\n",
    "    trans_dataset = get_word_translation_dataset(\n",
    "        target_lang, ulist(lang_pairs.flatten()), v2=True\n",
    "    )\n",
    "    full_def_dataset = get_cloze_dataset(\n",
    "        output_langs + [target_lang], drop_no_defs=True\n",
    "    )\n",
    "    # compute intersection between trans_dataset and def_dataset\n",
    "    common_concepts = set(trans_dataset[\"word_original\"]).intersection(\n",
    "        set(full_def_dataset[\"word_original\"])\n",
    "    )\n",
    "    if DEBUG:\n",
    "        common_concepts = sample(list(common_concepts), 8)\n",
    "        print(\"Debug mode: using only 8 common concepts\")\n",
    "    trans_dataset = trans_dataset[trans_dataset[\"word_original\"].isin(common_concepts)]\n",
    "    def_dataset = full_def_dataset[\n",
    "        full_def_dataset[\"word_original\"].isin(common_concepts)\n",
    "    ]\n",
    "    common_concepts = def_dataset[\"word_original\"].tolist()\n",
    "    assert len(trans_dataset) == len(def_dataset)\n",
    "    print(f\"Found {len(common_concepts)} common concepts\")\n",
    "    trans_source_prompts = np.array(\n",
    "        [\n",
    "            translation_prompts(\n",
    "                trans_dataset,\n",
    "                tokenizer,\n",
    "                input_lang,\n",
    "                output_lang,\n",
    "                n=num_few_shot,\n",
    "                cut_at_obj=True,\n",
    "            )\n",
    "            for input_lang, output_lang in lang_pairs\n",
    "        ]\n",
    "    )\n",
    "    trans_source_prompts_str = list(\n",
    "        chain.from_iterable(\n",
    "            [p.prompt for p in prompts] for prompts in trans_source_prompts\n",
    "        )\n",
    "    )\n",
    "    def_source_prompts = [\n",
    "        def_prompt(\n",
    "            def_dataset,\n",
    "            tokenizer,\n",
    "            output_lang,\n",
    "            n=num_few_shot,\n",
    "            use_word_to_def=True,\n",
    "            cut_at_obj=True,\n",
    "        )\n",
    "        for output_lang in output_langs\n",
    "    ]\n",
    "    def_source_prompts_str = list(\n",
    "        chain.from_iterable(\n",
    "            [p.prompt for p in prompts] for prompts in def_source_prompts\n",
    "        )\n",
    "    )\n",
    "\n",
    "    gt_defs, _ = ground_truth_defs(target_lang, common_concepts)\n",
    "    trans_activations = (\n",
    "        collect_activations_batched(\n",
    "            nn_model,\n",
    "            list(trans_source_prompts_str),\n",
    "            batch_size=batch_size,\n",
    "            tqdm=tqdm,\n",
    "        )\n",
    "        .transpose(0, 1)\n",
    "        .reshape(len(lang_pairs), len(trans_dataset), get_num_layers(nn_model), -1)\n",
    "    )  # num_langs, num_concepts, num_layers, model_dim\n",
    "    trans_mean_reprs = trans_activations.mean(\n",
    "        dim=0\n",
    "    )  # num_concepts, num_layers, model_dim\n",
    "    trans_single_reprs = trans_activations[0]  # num_concepts, num_layers, model_dim\n",
    "    def_activations = (\n",
    "        collect_activations_batched(\n",
    "            nn_model,\n",
    "            def_source_prompts_str,\n",
    "            batch_size=batch_size,\n",
    "            tqdm=tqdm,\n",
    "        )\n",
    "        .transpose(0, 1)\n",
    "        .reshape(len(output_langs), len(def_dataset), get_num_layers(nn_model), -1)\n",
    "    )  # num_langs, num_concepts, num_layers, model_dim\n",
    "    def_mean_reprs = def_activations.mean(dim=0)  # num_concepts, num_layers, model_dim\n",
    "    def_single_reprs = def_activations[0]  # num_concepts, num_layers, model_dim\n",
    "    target_prompts = []\n",
    "    for concept in common_concepts:\n",
    "        safe_df = full_def_dataset[full_def_dataset[\"word_original\"] != concept]\n",
    "        safe_prompt = sample(\n",
    "            def_prompt(\n",
    "                safe_df,\n",
    "                tokenizer,\n",
    "                target_lang,\n",
    "                n=num_few_shot,\n",
    "                use_word_to_def=True,\n",
    "                cut_at_obj=False,\n",
    "            ),\n",
    "            1,\n",
    "        )[0]\n",
    "        target_prompts.append(safe_prompt)\n",
    "    target_prompts_str = [p.prompt for p in target_prompts]\n",
    "    baseline_target_prompts_str = [\n",
    "        p.prompt\n",
    "        for p in def_prompt(\n",
    "            def_dataset,\n",
    "            tokenizer,\n",
    "            target_lang,\n",
    "            n=num_few_shot,\n",
    "            use_word_to_def=True,\n",
    "            cut_at_obj=False,\n",
    "        )\n",
    "    ]\n",
    "    other_concept_defs = [{} for _ in range(num_other_for_loss)]\n",
    "    for i, concept in enumerate(common_concepts):\n",
    "        other_concepts = th.randint(0, len(common_concepts) - 1, (num_other_for_loss,))\n",
    "        other_concepts[other_concepts >= i] += 1\n",
    "        for j, other_concept in enumerate(other_concepts):\n",
    "            other_concept_defs[j][concept] = gt_defs[common_concepts[other_concept]]\n",
    "    generations = {}\n",
    "    losses = {}\n",
    "    other_concepts_losses = [{} for _ in range(num_other_for_loss)]\n",
    "\n",
    "    for repr_type, reprs in [\n",
    "        (\"from def\", def_mean_reprs),\n",
    "        (\"from single def\", def_single_reprs),\n",
    "        (\"from trans\", trans_mean_reprs),\n",
    "        (\"from single trans\", trans_single_reprs),\n",
    "        (\"prompting\", None),\n",
    "    ]:\n",
    "        tgt_prompts = target_prompts_str\n",
    "        if repr_type == \"prompting\":\n",
    "            tgt_prompts = baseline_target_prompts_str\n",
    "        if generate_generations:\n",
    "            output = patched_generation(tgt_prompts, reprs, gen_batch_size)\n",
    "            generations[repr_type] = tokenizer.batch_decode(\n",
    "                output, skip_special_tokens=True\n",
    "            )\n",
    "        losses[repr_type] = loss_on_defs(tgt_prompts, gt_defs, reprs)\n",
    "        for i, other_defs in enumerate(other_concept_defs):\n",
    "            other_concepts_losses[i][repr_type] = loss_on_defs(\n",
    "                tgt_prompts, other_defs, reprs\n",
    "            )\n",
    "\n",
    "    json_file = path / (\"patched_generations_and_losses.json\")\n",
    "    defs = defaultdict(dict)\n",
    "    generations_dict = defaultdict(dict)\n",
    "    losses_dict = defaultdict(lambda: defaultdict(dict))\n",
    "    for repr_type in [\n",
    "        \"from def\",\n",
    "        \"from single def\",\n",
    "        \"from trans\",\n",
    "        \"from single trans\",\n",
    "        \"prompting\",\n",
    "    ]:\n",
    "        for i, concept in enumerate(common_concepts):\n",
    "            if generate_generations:\n",
    "                gen = generations[repr_type][i]\n",
    "                defs[concept][repr_type] = extract_def(gen)\n",
    "                generations_dict[concept][repr_type] = gen\n",
    "            losses_dict[concept][repr_type][\"other\"] = [\n",
    "                ocl[repr_type][concept] for ocl in other_concepts_losses\n",
    "            ]\n",
    "            losses_dict[concept][repr_type][\"normal\"] = losses[repr_type][concept]\n",
    "    json_dic = {\n",
    "        \"defs\": defs,\n",
    "        \"generations\": generations_dict,\n",
    "        \"losses\": losses_dict,\n",
    "    }\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(json_dic, f, indent=4)\n",
    "    return defs, losses_dict, other_concept_defs\n",
    "\n",
    "\n",
    "def generate_generations(prompts, concepts):\n",
    "    str_prompts = [p.prompt for p in prompts]\n",
    "    dataloader = DataLoader(str_prompts, batch_size=batch_size)\n",
    "    generations = []\n",
    "    for batch in dataloader:\n",
    "        with nn_model.generate(\n",
    "            batch, max_new_tokens=50, stop_strings=[\"\\n\"], tokenizer=nn_model.tokenizer\n",
    "        ):\n",
    "            out = nn_model.generator.output.tolist().save()\n",
    "        generations.extend(out)\n",
    "    generations = tokenizer.batch_decode(generations, skip_special_tokens=True)\n",
    "    generations = {\n",
    "        concept: \"...\\n\" + \"\\n\".join(generation.split(\"\\n\")[-3:-1])\n",
    "        for concept, generation in zip(concepts, generations)\n",
    "    }\n",
    "    defs = {\n",
    "        concept: extract_def(generation) for concept, generation in generations.items()\n",
    "    }\n",
    "    return {\"defs\": defs, \"generations\": generations}\n",
    "\n",
    "\n",
    "def word_patching_defs(\n",
    "    source_langs, target_lang, concepts, gt_defs, other_concept_defs, exp_path\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a definition in target_lang from the source_lang concept\n",
    "    \"\"\"\n",
    "    df = get_cloze_dataset(source_langs + [target_lang], drop_no_defs=True)\n",
    "    df = df[df[\"word_original\"].isin(concepts)]\n",
    "    assert len(df) == len(concepts)\n",
    "    prompts = []\n",
    "    for concept in concepts:\n",
    "        safe_df = df[df[\"word_original\"] != concept]\n",
    "        source_lang = sample(source_langs, 1)[0]\n",
    "        # add a new row with word_original = concept, senses_{target_lang} = senses_{source_lang}\n",
    "        original_row = df[df[\"word_original\"] == concept].iloc[0]\n",
    "        new_row = original_row.copy()\n",
    "        new_row[f\"senses_{target_lang}\"] = original_row[f\"senses_{source_lang}\"]\n",
    "        safe_df = pd.concat([safe_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        prompt = def_prompt(\n",
    "            safe_df,\n",
    "            tokenizer,\n",
    "            target_lang,\n",
    "            n=num_few_shot,\n",
    "            use_word_to_def=True,\n",
    "            concepts=[concept],\n",
    "        )\n",
    "        prompts.append(prompt[0])\n",
    "    result_dict = generate_generations(prompts, concepts)\n",
    "    losses = {\n",
    "        concept: {\"normal\": loss, \"other\": []}\n",
    "        for concept, loss in loss_on_defs([p.prompt for p in prompts], gt_defs).items()\n",
    "    }\n",
    "    other_concepts_losses = [\n",
    "        loss_on_defs([p.prompt for p in prompts], other_defs)\n",
    "        for other_defs in other_concept_defs\n",
    "    ]\n",
    "    for concept in concepts:\n",
    "        for other_losses in other_concepts_losses:\n",
    "            losses[concept][\"other\"].append(other_losses[concept])\n",
    "    json_file = exp_path / (\"word_patching_defs.json\")\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(result_dict, f, indent=4)\n",
    "    return result_dict[\"defs\"], losses\n",
    "\n",
    "\n",
    "# def gen_baseline_defs(lang, concepts, exp_path):\n",
    "#     df = get_cloze_dataset(lang, drop_no_defs=True)\n",
    "#     prompts = def_prompt(\n",
    "#         df, tokenizer, lang, n=num_few_shot, use_word_to_def=True, concepts=concepts\n",
    "#     )\n",
    "#     json_dic = generate_generations(prompts, concepts)\n",
    "#     json_file = exp_path / (exp_id + \"baseline_defs.json\")\n",
    "#     with open(json_file, \"w\") as f:\n",
    "#         json.dump(json_dic, f, indent=4)\n",
    "#     return json_dic[\"defs\"]\n",
    "\n",
    "\n",
    "def ground_truth_defs(lang, concept):\n",
    "    df = load_synset(lang)\n",
    "    gt_defs = {\n",
    "        concept: ulist(\n",
    "            ast.literal_eval(\n",
    "                df[df[\"word_original\"] == concept][\"definitions\"].tolist()[0]\n",
    "            )\n",
    "        )\n",
    "        for concept in concept\n",
    "    }\n",
    "    tgt_words = {\n",
    "        concept: ast.literal_eval(\n",
    "            df[df[\"word_original\"] == concept][\"senses\"].tolist()[0]\n",
    "        )[0]\n",
    "        for concept in concept\n",
    "    }\n",
    "    return gt_defs, tgt_words\n",
    "\n",
    "\n",
    "def generate_embeddings(prompts):\n",
    "    if isinstance(prompts, dict):\n",
    "        prompts = list(itertools.chain.from_iterable(prompts.values()))\n",
    "    embeddings = embeddings_model.encode(\n",
    "        prompts, batch_size=emb_batch_size, convert_to_tensor=True\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def compare_defs(defs, gt_defs_embeddings):\n",
    "    \"\"\"\n",
    "    Compare the definitions generated by the model to the ground truth definitions.\n",
    "\n",
    "    Args:\n",
    "        defs: a dictionary of concept -> list of definitions\n",
    "        gt_defs_embeddings: a tensor of shape (num_concepts, num_defs, embedding_dim) containing the embeddings of the ground truth definitions\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    all_defs = list(itertools.chain.from_iterable(defs.values()))\n",
    "    all_defs_embeddings = generate_embeddings(all_defs)\n",
    "    similarities = embeddings_model.similarity(all_defs_embeddings, gt_defs_embeddings)\n",
    "    return similarities\n",
    "\n",
    "\n",
    "def plot_results(result_dict, method_names, title_suffix, path):\n",
    "    # === Plotting ===\n",
    "    extra_path = path / \"extras\"\n",
    "    extra_path.mkdir(exist_ok=True)\n",
    "    for method, title in method_names.items():\n",
    "        plot_defs_comparison(result_dict, method, title, extra_path, show=False)\n",
    "    plot_compare_setup(result_dict, path, title_suffix.replace(\"<br>\", \"\"), exp_id)\n",
    "\n",
    "\n",
    "def experiment(lang_pairs, target_lang):\n",
    "    pref = \"_\".join(\"-\".join(ls) for ls in lang_pairs)\n",
    "    title_suffix = (\n",
    "        \"<br>(\" + \", \".join([t for s, t in lang_pairs]) + \") -> \" + target_lang\n",
    "    )\n",
    "    method_names = {\n",
    "        \"from trans\": \"Patching mean representation from translations\" + title_suffix,\n",
    "        \"from def\": \"Patching mean representation from definitions\" + title_suffix,\n",
    "        \"from single trans\": \"Patching single representation from translations\"\n",
    "        + title_suffix,\n",
    "        \"from single def\": \"Patching single representation from definitions\"\n",
    "        + title_suffix,\n",
    "        \"word patch\": \"Word patching\" + title_suffix,\n",
    "        \"prompting\": \"Vanilla prompting\" + f\" {target_lang}\",\n",
    "        \"repeat word\": \"Repeat word\" + f\" {target_lang}\",\n",
    "        \"rnd gt\": \"Random GT definition\" + f\" {target_lang}\",\n",
    "    }\n",
    "    methods = list(method_names.keys())\n",
    "    path = Path(\"results\") / model_name / exp_name / f\"{pref}-{target_lang}\" / exp_id\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    source_output_langs = ulist([p[1] for p in lang_pairs])\n",
    "    # === Generate Definitions ===\n",
    "    defs, losses, other_concept_defs = patched_repr_defs(lang_pairs, target_lang, path)\n",
    "    concepts = defs.keys()\n",
    "    gt_defs, tgt_words = ground_truth_defs(target_lang, concepts)\n",
    "    token_patching_defs, word_patching_losses = word_patching_defs(\n",
    "        source_output_langs, target_lang, concepts, gt_defs, other_concept_defs, path\n",
    "    )\n",
    "    for w in losses:\n",
    "        losses[w][\"word patch\"] = word_patching_losses[w]\n",
    "    with open(path / \"losses.json\", \"w\") as f:\n",
    "        json.dump(losses, f, indent=4)\n",
    "    dict_factory_losses = lambda: {m: defaultdict(dict) for m in methods[:-2]}\n",
    "    losses_dict = defaultdict(dict_factory_losses)\n",
    "    for w in losses:\n",
    "        for setup in losses[w]:\n",
    "            max_loss = max(losses[w][setup][\"normal\"])\n",
    "            losses_dict[w][setup][\"max loss\"] = max_loss\n",
    "            losses_dict[w][setup][\"mean loss\"] = np.mean(losses[w][setup][\"normal\"])\n",
    "            losses_dict[w][setup][\"min loss\"] = np.min(losses[w][setup][\"normal\"])\n",
    "            all_losses = list(chain.from_iterable(losses[w][setup][\"other\"]))\n",
    "            losses_dict[w][setup][\"mean loss with others\"] = np.mean(all_losses)\n",
    "            losses_dict[w][setup][\"mean max loss with others\"] = np.mean(\n",
    "                [max(other_losses) for other_losses in losses[w][setup][\"other\"]]\n",
    "            )\n",
    "            losses_dict[w][setup][\"mean min loss with others\"] = np.mean(\n",
    "                [min(other_losses) for other_losses in losses[w][setup][\"other\"]]\n",
    "            )\n",
    "    with open(path / \"losses_stats.json\", \"w\") as f:\n",
    "        json.dump(losses_dict, f, indent=4)\n",
    "    plot_losses_comparison(losses_dict, path, title_suffix)\n",
    "\n",
    "    for w_defs in gt_defs.values():\n",
    "        shuffle(w_defs)\n",
    "    all_defs = {\n",
    "        concept: [\n",
    "            defs[concept][\"from trans\"],\n",
    "            defs[concept][\"from def\"],\n",
    "            defs[concept][\"from single trans\"],\n",
    "            defs[concept][\"from single def\"],\n",
    "            token_patching_defs[concept],\n",
    "            defs[concept][\"prompting\"],\n",
    "            tgt_words[concept],\n",
    "            gt_defs[concept][0],\n",
    "        ]\n",
    "        for concept in defs\n",
    "    }\n",
    "\n",
    "    # === Generate Embeddings ===\n",
    "    gt_defs_embeddings = generate_embeddings(gt_defs)\n",
    "    all_idx = []\n",
    "    gt_embeddings_dict = {}\n",
    "    prev_idx = 0\n",
    "    for concept, defs_list in gt_defs.items():\n",
    "        gt_embeddings_dict[concept] = gt_defs_embeddings[\n",
    "            prev_idx : prev_idx + len(defs_list)\n",
    "        ]\n",
    "        prev_idx += len(defs_list)\n",
    "        all_idx.append(prev_idx)\n",
    "    all_idx.append(0)  # for the first concept\n",
    "\n",
    "    mean_embeddings = th.stack([emb.mean(dim=0) for emb in gt_embeddings_dict.values()])\n",
    "    mean_embeddings_wo_fst = th.stack(\n",
    "        [\n",
    "            emb[1:].mean(dim=0) if len(emb) > 1 else emb.mean(dim=0)\n",
    "            for emb in gt_embeddings_dict.values()\n",
    "        ]\n",
    "    )\n",
    "    assert (\n",
    "        mean_embeddings.shape[0] == len(defs)\n",
    "        and mean_embeddings.shape[1] == gt_defs_embeddings.shape[1]\n",
    "    ), f\"Shape mismatch: mean_embeddings.shape={mean_embeddings.shape} != gt_defs_embeddings.shape={gt_defs_embeddings.shape}, len(defs)={len(defs)}\"\n",
    "    similarities = compare_defs(all_defs, gt_defs_embeddings)\n",
    "    similarities_mean = compare_defs(all_defs, mean_embeddings)\n",
    "    similarities_mean_wo_fst = compare_defs(all_defs, mean_embeddings_wo_fst)\n",
    "\n",
    "    dict_factory = lambda: {m: {} for m in methods}\n",
    "\n",
    "    result_dict = defaultdict(dict_factory)\n",
    "    for i, concept in enumerate(defs):\n",
    "        for j, method in enumerate(methods):\n",
    "            sim_w_mean = similarities_mean[len(methods) * i + j][i]\n",
    "            sim_w_mean_wo_fst = similarities_mean_wo_fst[len(methods) * i + j][i]\n",
    "            start_idx = all_idx[i - 1]\n",
    "            if method == \"rnd gt\":\n",
    "                start_idx += 1  # skip self\n",
    "            sims = similarities[len(methods) * i + j][start_idx : all_idx[i]]\n",
    "            other_sims = th.cat(\n",
    "                [\n",
    "                    similarities[len(methods) * i + j][0 : all_idx[i - 1]],\n",
    "                    similarities[len(methods) * i + j][all_idx[i] :],\n",
    "                ]\n",
    "            )\n",
    "            max_sim_with_others = []\n",
    "            for k in range(len(defs)):\n",
    "                if k == i:\n",
    "                    continue\n",
    "                max_sim_with_others.append(\n",
    "                    similarities[len(methods) * i + j][all_idx[k - 1] : all_idx[k]]\n",
    "                    .max()\n",
    "                    .item()\n",
    "                )\n",
    "            result_dict[concept][method][\n",
    "                \"mean sim with others\"\n",
    "            ] = other_sims.mean().item()\n",
    "            result_dict[concept][method][\"mean max sim with others\"] = np.mean(\n",
    "                max_sim_with_others\n",
    "            )\n",
    "            if method == \"rnd gt\" and len(gt_defs[concept]) == 1:\n",
    "                # doesn't make sense to compare to itself\n",
    "                result_dict[concept][method][\"sim w mean\"] = None\n",
    "                result_dict[concept][method][\"sim w mean fst\"] = None\n",
    "                result_dict[concept][method][\"mean sim\"] = None\n",
    "                result_dict[concept][method][\"max sim\"] = None\n",
    "            else:\n",
    "                result_dict[concept][method][\"sim w mean\"] = sim_w_mean.item()\n",
    "                result_dict[concept][method][\n",
    "                    \"sim w mean fst\"\n",
    "                ] = sim_w_mean_wo_fst.item()\n",
    "                result_dict[concept][method][\"mean sim\"] = sims.mean().item()\n",
    "                result_dict[concept][method][\"max sim\"] = sims.max().item()\n",
    "\n",
    "    json_file = path / (\"defs_comparison.json\")\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(result_dict, f, indent=4)\n",
    "    plot_results(result_dict, method_names, title_suffix, path)\n",
    "    return result_dict, method_names, title_suffix, path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_pairs = [[\"de\", \"it\"], [\"nl\", \"fi\"], [\"zh\", \"es\"], [\"es\", \"ru\"], [\"ru\", \"ko\"]]\n",
    "paper_ins = [\"de\", \"nl\", \"zh\", \"es\", \"ru\"]\n",
    "test_pairs = [[\"es\", \"fr\"], [\"es\", \"de\"]]\n",
    "hard_langs = [\"ko\", \"ja\", \"et\", \"fi\"]\n",
    "easy_langs = [\"en\", \"fr\", \"zh\", \"de\"]\n",
    "paper_args = [\n",
    "    ([[\"es\", \"fr\"], [\"es\", \"de\"]], \"en\"),\n",
    "    (paper_pairs, \"zh\"),\n",
    "    ([[\"it\", \"es\"], [\"it\", \"de\"]], \"fr\"),\n",
    "    (paper_pairs, \"en\"),\n",
    "    ([[\"en\", hl] for hl in hard_langs], \"fr\"),\n",
    "    ([[\"hi\", el] for el in easy_langs], \"et\"),\n",
    "]\n",
    "if DEBUG:\n",
    "    paper_args = [paper_args[0]]\n",
    "for pargs in paper_args:\n",
    "    try:\n",
    "        plot_args = experiment(*pargs)\n",
    "    finally:\n",
    "        gc.collect()\n",
    "        th.cuda.empty_cache()\n",
    "        th.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}