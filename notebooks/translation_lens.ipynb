{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERACTIVE_MODE = False  # Set to True to run the notebook interactively\n",
    "\n",
    "import sys\n",
    "\n",
    "if INTERACTIVE_MODE:\n",
    "    sys.path.append(\"../src\")\n",
    "    %load_ext autoreload\n",
    "    %autoreload 3\n",
    "else:\n",
    "    sys.path.append(\"./src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "_ = th.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"translation_lens\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Papermill args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# papermill parameters\n",
    "batch_size = 8\n",
    "model = \"meta-llama/Llama-2-7b-hf\"\n",
    "model_path = None\n",
    "trust_remote_code = False\n",
    "device = \"auto\"\n",
    "remote = False\n",
    "extra_args = []\n",
    "exp_id = None\n",
    "num_few_shot = 5\n",
    "use_tl = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CL Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--lens\", type=str, default=\"patchscope\")\n",
    "parser.add_argument(\"--thinking-langs\", \"-t\", type=str, nargs=\"*\", default=[\"en\"])\n",
    "parser.add_argument(\"--map-source-lang\", type=str, default=None)\n",
    "parser.add_argument(\"--map-source-lang_kwargs\", type=dict, default={})\n",
    "parser.add_argument(\"--map-target-lang\", type=str, default=None)\n",
    "parser.add_argument(\"--map-target-lang_kwargs\", type=dict, default={})\n",
    "parser.add_argument(\"--prob-treshold\", type=float, default=0.0)\n",
    "\n",
    "args = parser.parse_args(extra_args)\n",
    "print(f\"args: {args}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp_tools import load_model\n",
    "from interventions import logit_lens, patchscope_lens\n",
    "import prompt_tools\n",
    "from functools import partial\n",
    "\n",
    "model_name = model.split(\"/\")[-1]\n",
    "if model_path is None:\n",
    "    model_path = model\n",
    "nn_model = load_model(\n",
    "    model_path,\n",
    "    trust_remote_code=trust_remote_code,\n",
    "    device_map=device,\n",
    "    use_tl=use_tl,\n",
    ")\n",
    "tokenizer = nn_model.tokenizer\n",
    "\n",
    "\n",
    "if isinstance(args.map_source_lang, str):\n",
    "    args.map_source_lang = getattr(prompt_tools, args.map_source_lang)\n",
    "    args.map_source_lang = partial(args.map_source_lang, **args.map_source_lang_kwargs)\n",
    "if isinstance(args.map_target_lang, str):\n",
    "    args.map_target_lang = getattr(prompt_tools, args.map_target_lang)\n",
    "    args.map_target_lang = partial(args.map_target_lang, **args.map_target_lang_kwargs)\n",
    "\n",
    "lens_name = args.lens\n",
    "if lens_name == \"logit_lens\":\n",
    "    lens_func = logit_lens\n",
    "elif \"patchscope\" in args.lens:\n",
    "    lens_func = patchscope_lens\n",
    "else:\n",
    "    raise ValueError(f\"Invalide method:  {args.lens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp_tools import (\n",
    "    run_prompts,\n",
    "    filter_prompts_by_prob,\n",
    "    remove_colliding_prompts,\n",
    ")\n",
    "from prompt_tools import translation_prompts\n",
    "from load_dataset import get_word_translation_dataset as get_translations\n",
    "\n",
    "from display_utils import (\n",
    "    plot_topk_tokens,\n",
    "    k_subplots,\n",
    "    plot_results,\n",
    "    plot_k_results,\n",
    ")\n",
    "\n",
    "\n",
    "def plot_lens_results(\n",
    "    input_lang,\n",
    "    output_lang,\n",
    "    thinking_langs=None,\n",
    "    batch_size=batch_size,\n",
    "    num_words=None,\n",
    "    exp_id=None,\n",
    "    num_examples=9,\n",
    "):\n",
    "    if thinking_langs is None:\n",
    "        thinking_langs = []\n",
    "    thinking_langs = [l for l in thinking_langs if l != output_lang]\n",
    "    df = get_translations(\n",
    "        input_lang,\n",
    "        [output_lang, *thinking_langs],\n",
    "        num_words=num_words,\n",
    "    )\n",
    "    no_proc = []\n",
    "    if args.map_source_lang is not None:\n",
    "        df[input_lang + \" no proc\"] = df[input_lang]\n",
    "        df[input_lang] = df[input_lang].apply(args.map_source_lang)\n",
    "        no_proc.append(input_lang + \" no proc\")\n",
    "    if args.map_target_lang is not None:\n",
    "        df[output_lang + \" no proc\"] = df[output_lang]\n",
    "        df[output_lang] = df[output_lang].apply(args.map_target_lang)\n",
    "        no_proc.append(output_lang + \" no proc\")\n",
    "    target_prompts = translation_prompts(\n",
    "        df,\n",
    "        tokenizer,\n",
    "        input_lang,\n",
    "        output_lang,\n",
    "        thinking_langs + no_proc,\n",
    "        augment_tokens=False,\n",
    "        n=num_few_shot,\n",
    "    )\n",
    "    target_prompts = remove_colliding_prompts(target_prompts, ignore_langs=no_proc)\n",
    "    print(f\"Number of non-colliding prompts: {len(target_prompts)}\")\n",
    "    target_prompts = filter_prompts_by_prob(\n",
    "        target_prompts, nn_model, args.prob_treshold, batch_size=batch_size\n",
    "    )\n",
    "    print(f\"Number of prompts after filtering: {len(target_prompts)}\")\n",
    "    if len(target_prompts) < num_examples:\n",
    "        print(\"Not enough prompts after filtering\")\n",
    "        return\n",
    "\n",
    "    if len(target_prompts) == 0:\n",
    "        print(f\"No prompts left after filtering for {input_lang} -> {output_lang}\")\n",
    "        return\n",
    "\n",
    "    target_probs, latent_probs = run_prompts(\n",
    "        nn_model,\n",
    "        target_prompts,\n",
    "        batch_size=batch_size,\n",
    "        get_probs=lens_func,\n",
    "        get_probs_kwargs=dict(remote=remote),\n",
    "    )\n",
    "\n",
    "    json_dic = {\n",
    "        output_lang: target_probs.tolist(),\n",
    "    }\n",
    "    for label, probs in latent_probs.items():\n",
    "        json_dic[label] = probs.tolist()\n",
    "    path = (\n",
    "        Path(\"results\")\n",
    "        / model_name\n",
    "        / exp_name\n",
    "        / (f\"{pref}-{input_lang}_{output_lang}-\")\n",
    "    )\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    json_file = path / (exp_id + \".json\")\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(json_dic, f, indent=4)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    pref = pref.replace(\"_\", \" \")\n",
    "    title = f\"{model_name}: {lens_name} for {input_lang} -> {output_lang}\"\n",
    "    plot_results(ax, target_probs, latent_probs, output_lang)\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    plot_file = path / (exp_id + \".png\")\n",
    "    plt.savefig(plot_file, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot k examples\n",
    "    fig, axes = k_subplots(num_examples)\n",
    "    plot_k_results(axes, target_probs, latent_probs, output_lang, num_examples)\n",
    "    axes[num_examples - 1].legend()\n",
    "    fig.suptitle(title)\n",
    "    plt_file = path / (exp_id + \"_k.png\")\n",
    "    fig.savefig(plt_file, dpi=300, bbox_inches=\"tight\")\n",
    "    fig.show()\n",
    "    # Compute a single example\n",
    "    json_meta = {}\n",
    "    for i in range(num_examples):\n",
    "        json_meta[i] = {\n",
    "            \"input lang\": input_lang,\n",
    "            \"target lang\": output_lang,\n",
    "            \"target prompt\": target_prompts[i].prompt,\n",
    "            \"target prompt target\": target_prompts[i].target_strings,\n",
    "            \"target prompt latent\": target_prompts[i].latent_strings,\n",
    "        }\n",
    "    json_df = pd.DataFrame(json_meta)\n",
    "    with pd.option_context(\n",
    "        \"display.max_colwidth\",\n",
    "        None,\n",
    "        \"display.max_columns\",\n",
    "        None,\n",
    "        \"display.max_rows\",\n",
    "        None,\n",
    "    ):\n",
    "        display(json_df)\n",
    "    target_prompt_batch = [p.prompt for p in target_prompts[:num_examples]]\n",
    "    probs = lens_func(\n",
    "        nn_model,\n",
    "        target_prompt_batch,\n",
    "        scan=True,\n",
    "        remote=remote,\n",
    "    )\n",
    "    file = path / (exp_id + \"_heatmap.png\")\n",
    "    plot_topk_tokens(probs, nn_model, title=title, file=file)\n",
    "\n",
    "    meta_file = path / (exp_id + \"_heatmap.meta.json\")\n",
    "    with open(meta_file, \"w\") as f:\n",
    "        json.dump(json_meta, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected args for the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_args = [[\"de\", \"it\"]]\n",
    "for f_args in paper_args:\n",
    "    th.cuda.empty_cache()\n",
    "    plot_lens_results(*f_args, exp_id=exp_id, thinking_langs=args.thinking_langs.copy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
