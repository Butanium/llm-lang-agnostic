{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERACTIVE_MODE = False  # Set to True to run the notebook interactively\n",
    "\n",
    "import sys\n",
    "\n",
    "if INTERACTIVE_MODE:\n",
    "    sys.path.append(\"../src\")\n",
    "    %load_ext autoreload\n",
    "    %autoreload 3\n",
    "# else:\n",
    "sys.path.append(\"./src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "_ = th.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import CONFIG\n",
    "\n",
    "CONFIG.set_default_api_key(\"2efc189f6ff94f36b7cd316bbad080b9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# papermill parameters\n",
    "# batch_size = 64\n",
    "batch_size = 128\n",
    "model = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "model_path = None\n",
    "# model_path = \"/dlabscratch1/public/llm_weights/llama3.1_hf/Meta-Llama-3.1-8B/\"\n",
    "trust_remote_code = False\n",
    "device = \"auto\"\n",
    "# remote = True\n",
    "remote = True\n",
    "num_few_shot = 5\n",
    "exp_id = None\n",
    "extra_args = []\n",
    "use_tl = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnterp import load_model\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--langs\", default=[\"en\"], nargs=\"+\")\n",
    "parser.add_argument(\"--features\", default=[\"gender\"], nargs=\"+\")\n",
    "parser.add_argument(\"--num_pairs\", default=64, type=int)\n",
    "pargs = parser.parse_args(extra_args)\n",
    "\n",
    "\n",
    "if model_path is None:\n",
    "    model_path = model\n",
    "nn_model = load_model(\n",
    "    model_path,\n",
    "    trust_remote_code=trust_remote_code,\n",
    "    device_map=device,\n",
    "    use_tl=use_tl,\n",
    "    no_space_on_bos=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnterp.interventions import TargetPromptBatch, patchscope_lens\n",
    "from exp_tools import run_prompts\n",
    "from prompt_tools import (\n",
    "    feature_prompts,\n",
    "    get_shifted_prompt_pairs,\n",
    "    NotEnoughPromptsError,\n",
    ")\n",
    "\n",
    "from load_dataset import get_feature_dataset\n",
    "\n",
    "from utils import ulist\n",
    "from display_utils import plot_k_results, plot_topk_tokens, plot_results, k_subplots\n",
    "\n",
    "\n",
    "def shifted_feature_plot(\n",
    "    source_lang,\n",
    "    target_lang,\n",
    "    feature=\"gender\",\n",
    "    batch_size=batch_size,\n",
    "    num_pairs=200,\n",
    "    exp_id=exp_id,\n",
    "    k=4,\n",
    "    remote=remote,\n",
    "    layers=None,\n",
    "    num_few_shot=num_few_shot,\n",
    "):\n",
    "    \"\"\"\n",
    "    Patchscope with source hidden from:\n",
    "    index -1 and Prompt = source_input_lang: A -> source_target_lang:\n",
    "    Into target prompt:\n",
    "    into index = -1, prompt = input_lang: A -> target_lang:\n",
    "    Then plot with latent_langs, target_lang, source_target_lang\n",
    "    \"\"\"\n",
    "    model_name = model.split(\"/\")[-1]\n",
    "    global source_df, target_df, target_prompts, target_probs, latent_probs, source_prompts, _source_prompts, _target_prompts, source_prompts_str\n",
    "    if exp_id is None:\n",
    "        exp_id = str(int(time()))\n",
    "    else:\n",
    "        exp_id = str(exp_id)\n",
    "    source_df = get_feature_dataset(\n",
    "        feature,\n",
    "        source_lang,\n",
    "    )\n",
    "    target_df = get_feature_dataset(\n",
    "        feature,\n",
    "        target_lang,\n",
    "    )\n",
    "\n",
    "    _source_prompts = feature_prompts(\n",
    "        source_df,\n",
    "        source_lang,\n",
    "        nn_model.tokenizer,\n",
    "        augment_tokens=False,\n",
    "        ignore_start_of_word=True,\n",
    "        n=num_few_shot,\n",
    "    )\n",
    "    _target_prompts = feature_prompts(\n",
    "        target_df,\n",
    "        target_lang,\n",
    "        nn_model.tokenizer,\n",
    "        augment_tokens=False,\n",
    "        ignore_start_of_word=True,\n",
    "        n=num_few_shot,\n",
    "    )\n",
    "    try:\n",
    "        source_prompts, target_prompts = get_shifted_prompt_pairs(\n",
    "            source_df,\n",
    "            target_df,\n",
    "            _source_prompts,\n",
    "            _target_prompts,\n",
    "            None,\n",
    "            \"corr\" if source_lang != target_lang else None,\n",
    "            None,\n",
    "            f\"cfact {source_lang}\" if source_lang != target_lang else None,\n",
    "            [],\n",
    "            num_pairs,\n",
    "            merge_extra_langs=True,\n",
    "            label_col=\"label\",\n",
    "        )\n",
    "    except NotEnoughPromptsError:\n",
    "        return\n",
    "    source_prompts_str = [p.prompt for p in source_prompts]\n",
    "\n",
    "    def transverse_patchscope(nn_model, prompt_batch, scan=False, remote=remote):\n",
    "        offset = transverse_patchscope.offset\n",
    "        target_pathscope_prompts = TargetPromptBatch.from_prompts(prompt_batch, -1)\n",
    "        source_prompt_batch = source_prompts_str[offset : offset + len(prompt_batch)]\n",
    "        transverse_patchscope.offset += len(prompt_batch)\n",
    "        return patchscope_lens(\n",
    "            nn_model,\n",
    "            source_prompt_batch,\n",
    "            target_pathscope_prompts,\n",
    "            remote=remote,\n",
    "            layers=layers,\n",
    "        )\n",
    "\n",
    "    transverse_patchscope.offset = 0\n",
    "    target_probs, latent_probs = run_prompts(\n",
    "        nn_model,\n",
    "        target_prompts,\n",
    "        batch_size=batch_size,\n",
    "        get_probs=transverse_patchscope,\n",
    "        # remote=remote,\n",
    "    )\n",
    "\n",
    "    # Get the baseline to normalize the plots\n",
    "    source_prompts_probs, _ = run_prompts(\n",
    "        nn_model,\n",
    "        source_prompts,\n",
    "        batch_size=batch_size,\n",
    "        get_probs_kwargs=dict(remote=remote),\n",
    "    )\n",
    "    target_prompts_probs, _ = run_prompts(\n",
    "        nn_model,\n",
    "        target_prompts,\n",
    "        batch_size=batch_size,\n",
    "        get_probs_kwargs=dict(remote=remote),\n",
    "    )\n",
    "\n",
    "    json_dic = {\n",
    "        target_lang: target_probs.tolist(),\n",
    "        \"source prompt probs\": source_prompts_probs.squeeze().tolist(),\n",
    "        \"target prompt probs\": target_prompts_probs.squeeze().tolist(),\n",
    "    }\n",
    "    for label, probs in latent_probs.items():\n",
    "        json_dic[label] = probs.tolist()\n",
    "    path = (\n",
    "        Path(\"results\")\n",
    "        / model_name\n",
    "        / \"shifted_feature\"\n",
    "        / feature\n",
    "        / (f\"{source_lang}-{target_lang}\")\n",
    "    )\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    json_file = path / (exp_id + \".json\")\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(json_dic, f, indent=4)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    # Raw probabilities plot\n",
    "    plot_results(\n",
    "        ax,\n",
    "        target_probs,\n",
    "        latent_probs,\n",
    "        target_lang,\n",
    "        source_baseline=source_prompts_probs.mean(),\n",
    "        target_baseline=target_prompts_probs.mean(),\n",
    "    )\n",
    "    ax.legend()\n",
    "    title = (\n",
    "        f\"{model_name}: HeteroPatch for {feature} from {source_lang} into {target_lang}\"\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    plot_file = path / (exp_id + \".png\")\n",
    "    plt.savefig(plot_file, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot k examples\n",
    "    fig, axes = k_subplots(k)\n",
    "    plot_k_results(axes, target_probs, latent_probs, target_lang, k=k)\n",
    "    axes[k - 1].legend()\n",
    "    fig.suptitle(title)\n",
    "    plt_file = path / (exp_id + \"_k.png\")\n",
    "    fig.savefig(plt_file, dpi=300, bbox_inches=\"tight\")\n",
    "    fig.show()\n",
    "    # Compute a single example\n",
    "    json_meta = {}\n",
    "    for i in range(k):\n",
    "        json_meta[i] = {\n",
    "            \"source lang\": source_lang,\n",
    "            \"target lang\": target_lang,\n",
    "            \"source prompt\": source_prompts_str[i],\n",
    "            \"source prompt target\": source_prompts[i].target_strings,\n",
    "            \"source prompt latent\": source_prompts[i].latent_strings,\n",
    "            \"target prompt\": target_prompts[i].prompt,\n",
    "            \"target prompt target\": target_prompts[i].target_strings,\n",
    "            \"target prompt latent\": target_prompts[i].latent_strings,\n",
    "        }\n",
    "    json_df = pd.DataFrame(json_meta)\n",
    "    with pd.option_context(\n",
    "        \"display.max_colwidth\",\n",
    "        None,\n",
    "        \"display.max_columns\",\n",
    "        None,\n",
    "        \"display.max_rows\",\n",
    "        None,\n",
    "    ):\n",
    "        display(json_df)\n",
    "    target_prompt_batch = TargetPromptBatch.from_prompts(\n",
    "        [p.prompt for p in target_prompts[:k]], -1\n",
    "    )\n",
    "    probs = patchscope_lens(\n",
    "        nn_model, source_prompts_str[:k], target_prompt_batch, remote=remote\n",
    "    )\n",
    "    file = path / (exp_id + \"_heatmap.png\")\n",
    "    plot_topk_tokens(probs, nn_model.tokenizer, title=title, file=file)\n",
    "\n",
    "    meta_file = path / (exp_id + \"_heatmap.meta.json\")\n",
    "    with open(meta_file, \"w\") as f:\n",
    "        json.dump(json_meta, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_dataset import NoDatasetFound\n",
    "for source_lang in pargs.langs:\n",
    "    for target_lang in pargs.langs:\n",
    "        for feature in pargs.features:\n",
    "            try:\n",
    "                shifted_feature_plot(\n",
    "                    source_lang,\n",
    "                    target_lang,\n",
    "                    feature=feature,\n",
    "                    batch_size=batch_size,\n",
    "                    num_pairs=pargs.num_pairs,\n",
    "                    exp_id=exp_id,\n",
    "                    remote=remote,\n",
    "                )\n",
    "            except NoDatasetFound as e:\n",
    "                print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
